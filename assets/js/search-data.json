{
  
    
        "post0": {
            "title": "Metaprogramming aspects of fastai2",
            "content": "encode for model, decode for yourself . Conventionally, in PyTorch, we write most of the data pre-processing logic in a single block of code, IE, inside __getitem__ of PyTorch Dataset. Even if we try to create separate methods for different parts of the Pipeline, we often end up creating highly coupled logic, which works best for your current project, but you often hesitate to reuse this code for your next project. . class WhaleDataset(Dataset): def __init__(self, datafolder, datatype=&#39;train&#39;, df=None, transform = transforms.Compose([transforms.ToTensor()]), y=None): self.datafolder = datafolder self.datatype = datatype self.y = y if self.datatype == &#39;train&#39;: self.df = df.values self.image_files_list = [s for s in os.listdir(datafolder)] self.transform = transform def __len__(self): return len(self.image_files_list) def __getitem__(self, idx): if self.datatype == &#39;train&#39;: img_name = os.path.join(self.datafolder, self.df[idx][0]) label = self.y[idx] elif self.datatype == &#39;test&#39;: img_name = os.path.join(self.datafolder, self.image_files_list[idx]) label = np.zeros((5005,)) image = Image.open(img_name).convert(&#39;RGB&#39;) image = self.transform(image) if self.datatype == &#39;train&#39;: return image, label elif self.datatype == &#39;test&#39;: # so that the images will be in a correct order return image, label, self.image_files_list[idx] . The problem with this approach is a lack of modularity and the ability to test individual block of code. Certain problems that might be left unnoticed with this design are (Considering image classification problem): . You displayed a single image from given filenames, are you sure the same image is not being repeated in a batch? | You got the numeric labels from given text labels, but are you sure they‚Äôre mapped properly, do you know their reverse mapping? | how you tried several permutations of your given augmentations? What if your subject is getting cropped out of the image and you left wondering why my model isn‚Äôt converging ü§î | Are you sure that your split logic isn‚Äôt mixing the train and validation set? | We need a better way to organize the Transforms and fastai-v2 has introduced a novel way for this. A Transform class in fastai2 has a special behavior due to its encodes, decodes, and setup methods. . . As name suggests, encodes hold primary logic of transforming the input, which next transform will grab as input, process it and pass-on to the next. Similarly, decodes has the logic of undoing the effect of current transform. An ideal use-case for decodes is to manage the inconsistencies between different libraries. . For instance, in object detection, suppose your math library uses a matrix notations for coordinates, so model will be trained to predict the bounding in that form, but your visualization library uses the exact opposite, so you want to undo the preprocessing step while showing the results and decodes is the place to keep that logic. . Key features ofTransform : . type-dispatch: yes ! you can define type-specific methods in Python :astonished: what do you think TensorImage is in the above image.? Here is the definition | class TensorImage(TensorImageBase): pass . TensorImageBase is just a wrapper around a PyTorch Tensor holding image channels and TensorImage is just an empty class. We call it semantic type, as this type has some special treatment in the Pipeline. But the advantage is, now you can define a Transform (augmentation / preprocessing) for TensorImage, test that in isolation and just freeze that block of code in your library, no need to write that again in __getitem__ . Handling tuples: you just pass in tuple of whatever, Transform will act on only types it‚Äôs allowed to (all instances of that type). . | Reversibility: As discussed earlier, you can define how to undo the operation performed on a Tensor . | Ordering: you can define some order (int) for a Transform and they‚Äôll be applied in the ascending order of that. Suppose, you want some augmentation to be applied before Normalization, just specify the order of your Transform lower than Normalize and it‚Äôll be applied in desired order. . You can learn more about Transform here. Let‚Äôs look at an example: . | class Normalize(Transform): &quot;Normalize/denorm batch of `TensorImage`&quot; order=99 ... def setups(self, dl:DataLoader): if self.mean is None or self.std is None: x,*_ = dl.one_batch() self.mean,self.std = x.mean(self.axes, keepdim=True),x.std(self.axes, keepdim=True)+1e-7 def encodes(self, x:TensorImage): return (x-self.mean) / self.std def decodes(self, x:TensorImage): f = to_cpu if x.device.type==&#39;cpu&#39; else noop return (x*f(self.std) + f(self.mean)) ... . Normalize is one of the transforms offered by fastai. The setups method is used to perform one-time calculations such as mean and standard deviation in this case. encodes is just a replacement for __call__ while decodes is denormalizing the Tensor. Notice the type-annotation TensorImage which has a lot more meaning than just a typing hint offered by Python 3.7. Transform also retains the types, IE, an input TensorImage will be returned as TensorImage only and all this is handled by a meta-class for Transform . Method Overloading and Type Dispatch . class IntToFloatTensor(Transform): &quot;Transform image to float tensor, optionally dividing by 255 (e.g. for images).&quot; order = 10 #Need to run after PIL transforms on the GPU def __init__(self, div=255., div_mask=1): store_attr(self, &#39;div,div_mask&#39;) def encodes(self, o:TensorImage): return o.float().div_(self.div) def encodes(self, o:TensorMask ): return o.long() // self.div_mask def decodes(self, o:TensorImage): return ((o.clamp(0., 1.) * self.div).long()) if self.div else o . IntToFloatTensor has a separate behavior for TensorImage and TensorMask and this is achieved by dynamic type-dispatch. So if the type is TensorImage, it‚Äôll be divided by 255 and returned as a float Tensor, whereas Masks won‚Äôt be scaled and returned as long Tensors. Now, say, you want to return a double() Tensor instead of float and add some $ epsilon$=1e-7 to it, you can simply do that using @IntToFloatTensor decorator. . @IntToFloatTensor def encodes(self, x: TensorImage): return x.double().div_(self.div) + 1e-7 . which means, you don‚Äôt need to write a class extending Transform, if it‚Äôs some variation of existing one in the library, just write type-annotated encodes with a decorator of that method. . You don‚Äôt necessarily need extend the Transform class to make type-dispatch work. There‚Äôs a decorator for that as well . @typedispatch def show_results(x:TensorImage, y:(TensorMask, TensorPoint, TensorBBox), ...): @typedispatch def show_results(x:TensorImage, y:TensorCategory, ...): @typedispatch def show_results(x:TensorImage, y:TensorImage, ...): . fastai has utility functions to show the training results. Now, each combination has a different way of showing the results. For example, TensorImage and TensorCategory will simply show the image with target vs predicted label as title. A TensorImage with any localization type will actually show both ground-truth and predicted images. . . In this case, a ground-truth segmentation mask (left) and predicted (right) mask is shown as result. Look how everything is coming together and those semantic types are the actors in this plot :sunglasses: . @patch and @patch-property . There are several practices used to extend the functionality of existing class in the library. Swift or Kotlin does this by writing extension functions. ‚ÄúMonkey-patching‚Äù is one of those aspects used by several python libraries and fastai provides a decorator for that. A cool example of this: . @patch def pca(x:Tensor, k=2): &quot;Compute PCA of `x` with `k` dimensions.&quot; x = x-torch.mean(x,0) U,S,V = torch.svd(x.t()) return torch.mm(x,U[:,:k]) . We‚Äôre monkey-patching a method to compute PCA of given Tensor which allows us to call this method as if it was part of PyTorch tensors; so you can simply call x.pca() and it‚Äôll work like a charm. Another great example of this, if you‚Äôre coming from numpy background, you might be used to the shape property of arrays. This is not available in PyTorch, but not anymore, just patch it to the Tensor class and you‚Äôre good to go: . @patch_property def shape(x: Image.Image): return x.size[1],x.size[0] . Another example that I couldn‚Äôt resist to mention, you‚Äôve a ls() method for Path class, thanks to @patch :wink: . You can @delegate the rest . Say you are writing a wrapper for library function to add some tweaks and you‚Äôre only concerned with some parameters, rest of them will be passed on to the original method, how can you make sure that user should be able to work with every-single-parameter of original method. You need to write all those parameters in your method definition, don‚Äôt you?. Let‚Äôs work with matplotlib for this: . matplotlib.pyplot.plot has several customization parameters for graph, but we don‚Äôt really know all of them. As per matplotlib documentation, these are the properties of Line2D class, however, as matplotlib uses **kwargs to handle them, it remains mystery what all we can modify: . . This is the type-hint you get in the IDE for plt.plot() method and clearly most of the arguments are disguised in the **kwargs. I wrote a simple wrapper function for this which delegates those kwargs to Line2D. . @delegates(matplotlib.pyplot.Line2D) def mat_plot(x, **kwargs): return plt.plot(x,**kwargs) . And‚Ä¶. the result is: . . :frowning: we have these many arguments to play with!! . But @delegates isn‚Äôt just used to make those type-hints appear for you, you can have your own parameters along with those **kwargs. . in simple terms, your method doesn‚Äôt actually accept the kwargs, but the only parameters that are available in the method you‚Äôre delegating to. . For example, in fastai, there‚Äôs a method called save_model which has the logic to save the PyTorch model to given path. Now Learner has the ‚Äúmodel‚Äù, ‚Äúoptimizer state‚Äù and some parameters which could be used to create the ‚Äúdestination path‚Äù for save_model, so Learner will build the desired path and hand over its available parameters to save_model to perform actual save: . @delegates(save_model) def save(self, file, **kwargs): file = join_path_file(file, self.path/self.model_dir, ext=&#39;.pth&#39;) save_model(file, self.model, getattr(self,&#39;opt&#39;,None), **kwargs) . @delegates could also be used with classes. With no target specified, it‚Äôll delegate the parameters from you constructor to the base class‚Äôs one. . Who likes boilerplate? . Some decorators that makes you write even lesser code with Python :bangbang: . . Now we&#39;re manipulating the signature of class/method Do you want to control what could be passed in those **kwargs? This is not for your convenience :stuck_out_tongue:actually, end users would always like to know what they could pass in those **kwargs instead of guessing randomly. So it‚Äôs a deal benefiting both the parties. . @use_kwargs and @use_kwargs_dict . Making your method super flexible has no harm and who knows it may cover the use-case that you didn‚Äôt even think of. Also, cutting down the parameter list because it‚Äôs getting too long is not a great excuse either. So, you have a method with most of the kwargs being None and tired of listing all of them? or you‚Äôve already listed them somewhere else? then @use_kwargs is made for you. Borrowing this snippet from pytorch-metric-learning (modified a bit for brevity) . class Trainer: def __init__(self,models, loss_funcs, mining_funcs, iterations_per_epoch=None, data_device=None, loss_weights=None, sampler=None, collate_fn=None, lr_schedulers=None, gradient_clippers=None, data_and_label_getter=None, dataset_labels=None, end_of_iteration_hook=None, end_of_epoch_hook=None ): ... . of course there‚Äôs no harm in writing this way, but doesn‚Äôt it look too bulky? what if it grows even further? by the time you reach to actual logic for this method, the parameter list will already scare you off. Also, say you‚Äôve written this list somewhere else, why do you want to re-write here as well? @use_kwargs will make things a little cute for you :wink: . class Trainer: _none_attrs=&quot;&quot;&quot;iterations_per_epoch data_device loss_wights sampler collate_fn lr_schedulers gradient_clippers data_and_labels_getter dataset_labels end_of_iteration_hook end_of_epoch_hook&quot;&quot;&quot;.split() @use_kwargs(_none_attrs) def __init__(self,models,loss_funcs,mining_funcs,**kwargs): ... . But what if they‚Äôre not None and have their own default values :thinking: ? Borrowing this snippet from keras . class ImageDataGenerator(image.ImageDataGenerator): def __init__(self, featurewise_center=False, samplewise_center=False, featurewise_std_normalization=False, samplewise_std_normalization=False, zca_whitening=False, zca_epsilon=1e-6, rotation_range=0, width_shift_range=0., height_shift_range=0., brightness_range=None, shear_range=0., zoom_range=0., channel_shift_range=0., fill_mode=&#39;nearest&#39;, cval=0., horizontal_flip=False, vertical_flip=False, rescale=None, preprocessing_function=None, data_format=&#39;channels_last&#39;, validation_split=0.0, interpolation_order=1, dtype=&#39;float32&#39;): ... super(ImageDataGenerator, self).__init__( featurewise_center=featurewise_center, samplewise_center=samplewise_center, featurewise_std_normalization=featurewise_std_normalization, samplewise_std_normalization=samplewise_std_normalization, zca_whitening=zca_whitening, zca_epsilon=zca_epsilon, rotation_range=rotation_range, width_shift_range=width_shift_range, height_shift_range=height_shift_range, brightness_range=brightness_range, shear_range=shear_range, zoom_range=zoom_range, channel_shift_range=channel_shift_range, fill_mode=fill_mode, cval=cval, horizontal_flip=horizontal_flip, vertical_flip=vertical_flip, rescale=rescale, preprocessing_function=preprocessing_function, data_format=data_format, validation_split=validation_split, interpolation_order=interpolation_order, dtype=dtype) . :frowning: :cold_sweat: I‚Äôm not at all saying this bad, in fact, this follows the PEP8 coding standards; but the first question came into my mind was, what is image.ImageDataGenerator ? then I got to know, it came from another package called keras_preprocessing. So the next obvious question was, does that library has the exact same constructor, and indeed it has. Now, isn‚Äôt it redundant code? and what if, you‚Äôve some factory methods in the class that uses partial or exact same set of parameters? Can we do better? . class ImageDataGenerator2(IGenerator): _datagen_kwargs = dict(featurewise_center=False, samplewise_center=False, featurewise_std_normalization=False, samplewise_std_normalization=False, zca_whitening=False, zca_epsilon=1e-6, rotation_range=0, width_shift_range=0., height_shift_range=0., brightness_range=None, shear_range=0., zoom_range=0., channel_shift_range=0., fill_mode=&#39;nearest&#39;, cval=0., horizontal_flip=False, vertical_flip=False, rescale=None, preprocessing_function=None, data_format=&#39;channels_last&#39;, validation_split=0.0, interpolation_order=1, dtype=&#39;float32&#39;) @use_kwargs_dict(**_datagen_kwargs) def __init__(self,**kwargs): super(ImageDataGenerator, self).__init__(**kwargs) ... _auglist = &quot;&quot;&quot;brightness_range samplewise_center samplewise_std_normalization zca_epsilon zca_whitening&quot;&quot;&quot;.split() @classmethod @use_kwargs_dict(**dict((k,_datagen_kwargs[k]) for k in _auglist)) def augment(**kwargs): ... . At first, you might think what‚Äôs the difference :thinking: we ‚Äòre just writing it in a dictionary instead of method definition. But I see three advantages here: . The __init__ method is less scary and succinct. | We could reuse those parameters. As in the example, I‚Äôve created a static method augment which is using partial set of parameters from the _datagen_dict. | It became a central place for default arguments. Suppose, in future, we decided to change the data_format=channel_first; this will require you to change it from all the mentions of the same to make it consistent. Whereas, in this case, changing it in our default dictionary will be reflected in all the other places, making it less error prone. | . Note that, using **kwargs won&#39;t conceal the parameter list and you&#39;ll get all the parameters in type-completion . @funcs_kwargs . Now that we‚Äôre able handle kwargs in a better way, why not make the constructor accept methods as a parameter. Let‚Äôs define the default behavior for them, but let‚Äôs also allow the users to change it as well and that too, without inheritance. One reason to avoid inheritance is, doing so might break the dependency chain. We were not concerned about the types so far (as python is dynamic language) but now that I‚Äôve shown you the perks of type-dispatch, we need to worry about it a little. . Again, we won‚Äôt be polluting the argument list of __init__, instead we‚Äôve a contrived way for doing so :wink: . @funcs_kwargs class DataBlock(): &quot;Generic container to quickly build `Datasets` and `DataLoaders`&quot; get_x=get_items=splitter=get_y=None _methods = &#39;get_items splitter get_y get_x&#39;.split() def __init__(self, blocks=None, dl_type=None, getters=None, n_inp=None, item_tfms=None, batch_tfms=None, **kwargs): ... . By adding @funcs_kwargs decorator to the class definition, you‚Äôve enabled the users to modify any method you included in _methods list. For example, . dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(), get_y=parent_label, item_tfms=item_tfms, batch_tfms=batch_tfms) . parent_label is a function (simply returns the parent folder name) which is assigned to get_y. DataBlock makes get_y a no operation function unless you provide your implementation for that. (@funcs_kwargs are not shown in the auto-completion) . @docs in the source code :fire: . Writing documentation in the source code is absolutely rewarding for two reasons. . It makes your source code readable | Documentation generator libraries such as Sphinx, MkDocs, nbdev will generate the html docs for you | But writing it within method/class definition itself will make it so not readable. Just look at this Conv1D class :no_mouth:,‚Äã instead, grouping it at one place seems a better idea. . @docs class DataLoaders(GetAttr): _default=&#39;train&#39; _xtra=&quot;&quot; ... _docs=dict(__getitem__=&quot;Retrieve `DataLoader` at `i` (`0` is training, `1` is validation)&quot;, train=&quot;Training `DataLoader`&quot;, valid=&quot;Validation `DataLoader`&quot;, train_ds=&quot;Training `Dataset`&quot;, valid_ds=&quot;Validation `Dataset`&quot;, to=&quot;Use `device`&quot;, cuda=&quot;Use the gpu if available&quot;, cpu=&quot;Use the cpu&quot;, new_empty=&quot;Create a new empty version of `self` with the same transforms&quot;, from_dblock=&quot;Create a dataloaders from a given `dblock`&quot;) . Get it easily (GetAttr) . A wrapper class sometimes makes things redundant. GettAttr is meant to simplify this if you use the composed object frequently. Just specify that object as _default and no more redundancy. . dls = dblock.dataloaders(...) dls.train.n dls.train.vocab dls.train.tfms dls.train.show_batch() vs dls.n dls.vocab dls.tfms dls.show_batch() . But what if wrapper class also has a method/property with that name :thinking: ? then be specific about which properties you‚Äôd like to expose from the composed object. . class OptimWrapper(_BaseOptimizer, GetAttr): _xtra=[&#39;zero_grad&#39;, &#39;step&#39;, &#39;st ate_dict&#39;, &#39;load_state_dict&#39;] _default=&#39;opt&#39; . This wrapper is used to wrap PyTorch Optimizers and to add some utility functions associated with it. So the original optimizer is stored as opt in this class and we don‚Äôt really want to access all attributes like adam.opt.step, adam.opt.state_dict. Thus, class inherits GettAttr alongside _BaseOptimizer (multiple inheritance is allowed in Python). The _xtra parameter will make sure only those attributes will be passed on to the self. . These are the perquisites of dynamic Python and fastcore shows how malleable the language is. As I said in the beginning, you can start using all of these in your project as fastcore is independent of any Deep Learning framework. Hope you enjoyed the article, this is my first attempt to write such a thorough post, so your suggestions are most welcome! . References . fastcore | fastai2 | fastai paper | .",
            "url": "https://kshitij09.github.io/dev-blog/fastai2/fastcore/metaprogramming/2020/05/14/Meta-Programming-Aspects-of-fastai2.html",
            "relUrl": "/fastai2/fastcore/metaprogramming/2020/05/14/Meta-Programming-Aspects-of-fastai2.html",
            "date": " ‚Ä¢ May 14, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "PyTorch Exercises",
            "content": "Inspired by 100-numpy-exercises . 1. Import PyTorch and print version (&#9733;&#9734;&#9734;) . #collapse import torch torch.__version__ . . &#39;1.4.0&#39; . 2. Create a null vector of size 10 (&#9733;&#9734;&#9734;) . #collapse torch.zeros(10) . . tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) . 3. Create a vector with values ranging from 10 to 49 (&#9733;&#9734;&#9734;) . #collapse torch.arange(10,50) . . tensor([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]) . 4. Reverse a vector &amp; tensor (&#9733;&#9734;&#9734;) . #collapse #hide_output print(&quot;Vector&quot;) x = torch.arange(10) print(&quot;Original: &quot;,x) print(&quot;Reversed: &quot;, x.flip(0)) print(&quot;Tensor&quot;) x = x.view(5,2) print(&quot;Original: n&quot;,x) print(&quot;Reversed(rows): n&quot;, x.flip(0)) print(&quot;Reversed(cols): n&quot;, x.flip(1)) . . Vector Original: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) Reversed: tensor([9, 8, 7, 6, 5, 4, 3, 2, 1, 0]) Tensor Original: tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]) Reversed(rows): tensor([[8, 9], [6, 7], [4, 5], [2, 3], [0, 1]]) Reversed(cols): tensor([[1, 0], [3, 2], [5, 4], [7, 6], [9, 8]]) . 5. Create a 3x3 matrix with values ranging from 0 to 8 (&#9733;&#9734;&#9734;) . #collapse torch.arange(9).view(3,3) . . tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]]) . 6. Create a 3x3 Identity Matrix (&#9733;&#9734;&#9734;) . #collapse torch.eye(3) . . tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) . 7. Create a 3x3x3 matrix with random values (&#9733;&#9734;&#9734;) . (output hidden to avoid verbosity) . #collapse #hide_output # Random Uniform print(torch.rand(3,3)) # Random Normal print(torch.randn(3,3)) # Random int (low to high) print(torch.randint(1,10,(3,3))) # Random Permutations of given range print(torch.randperm(9).view(3,3)) . . tensor([[0.9199, 0.3884, 0.2975], [0.8924, 0.7694, 0.6850], [0.4952, 0.5713, 0.8097]]) tensor([[-2.1540, -0.4610, 0.3117], [-1.0630, -0.5528, -0.1765], [ 0.4584, 0.5330, 1.0833]]) tensor([[9, 3, 4], [9, 7, 4], [7, 1, 2]]) tensor([[8, 3, 7], [0, 4, 1], [6, 2, 5]]) . 8. Create a random vector of size 20 and find following stats: (&#9733;&#9734;&#9734;) . min,max, sum | mean, variance, standard deviation | . #collapse x = torch.randint(20,(20,)).float() print(x) x.min() , x.max(), x.sum(), x.mean(), x.std(), x.var() . . tensor([18., 15., 18., 4., 18., 16., 17., 2., 10., 16., 13., 1., 13., 8., 15., 10., 16., 17., 18., 9.]) . (tensor(1.), tensor(18.), tensor(254.), tensor(12.7000), tensor(5.4782), tensor(30.0105)) . 9. Create a 2d array with 1 on the border and 0 inside (&#9733;&#9734;&#9734;) . #collapse x = torch.ones(5,5) x[1:-1,1:-1] = 0 x . . tensor([[1., 1., 1., 1., 1.], [1., 0., 0., 0., 1.], [1., 0., 0., 0., 1.], [1., 0., 0., 0., 1.], [1., 1., 1., 1., 1.]]) . 10. How to add a border (filled with 0&#39;s) around an existing array? (&#9733;&#9734;&#9734;) . #collapse import torch.nn.functional as F x = torch.randn(3,3) F.pad(x,(1,1,1,1),&#39;constant&#39;,0) . . tensor([[ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.5841, 0.2092, -1.5766, 0.0000], [ 0.0000, -0.5721, -0.9200, -0.7613, 0.0000], [ 0.0000, -0.0597, -1.8524, 0.1481, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]) .",
            "url": "https://kshitij09.github.io/dev-blog/pytorch/2020/04/22/PyTorch-Exercises.html",
            "relUrl": "/pytorch/2020/04/22/PyTorch-Exercises.html",
            "date": " ‚Ä¢ Apr 22, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Software Development is what I used to pursue and was happy :smile: with my normal development cycle until I got introduced to Deep Learning. Now my project life-cycle is a stream of things that doesn‚Äôt-work doesn‚Äôt-work doesn‚Äôt-work until it eventually does :disappointed_relieved: . So my goal is to bring that rewarding cycle of Software Development to Deep Learning tasks. Particularly interested in High Level APIs that makes life easier and deep learning accessible to everyone. .",
          "url": "https://kshitij09.github.io/dev-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://kshitij09.github.io/dev-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}